### Schedule

**9:00 am**: Preparing with coffee

**9:15 am**: [Pair Problem](pair.md)

**10:00 am**: [Null Hypothesis](null_hypothesis_testing.md)

**10:15 am**: [Regularization](regularization.pdf)

**10:45 am**: [Regularization via Jupyter](Regularization.ipynb)

**12:00 am**: Essen

**1:30 pm**: Investigation Presentation

**1:45 pm**: Work on all things!


### Further "Reading"

 * [Regularized linear regression with scikit-learn](http://www.datarobot.com/blog/regularized-linear-regression-with-scikit-learn/): This goes over some of the theory we discussed, and shows using regularization on an actual scikit learn example. It uses Lasso instead of Ridge. The only difference between Lasso and Ridge regularization is this: Ridge adds the sum of beta squares to the cost, while Lasso adds sum of beta absolute values. Other than that functional form, the idea is pretty much the same. The interface of using LinearRegression() or Ridge(alpha) or Lasso(alpha) is also exactly the same.
 * [Ten minute video lecture on regularization](https://www.youtube.com/watch?v=fx-TqOzjDbM): Another ten minute lecture by Andrew Ng on how the cost function manipulation works in regularization. It helps build intuition.
 
